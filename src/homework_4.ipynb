{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TtThtm7iF8Gk"
      },
      "source": [
        "# \n",
        "\n",
        "- **Questions**: [Here](../data/homework_4/HW.pdf)\n",
        "- **Answer Set** : 04\n",
        "- **Full Name** : Fatemeh Karimi Barikarasfi\n",
        "- **Student Code** : 610301060\n",
        "\n",
        "The goal of this homework is IMDB Sentiment Classification using Implemented Transformer Encoder and Pre-Trained Bert Model.\n",
        "\n",
        "[Dataset](https://drive.google.com/drive/folders/1YJvoIpInw0fYz2fjHSR65mjs6IYFZUvN)\n",
        "\n",
        "## Importing Needed Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7xFW1nxOMYqQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pickle as pk\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as tfh\n",
        "import tensorflow_text as tft\n",
        "import tensorflow_models as tfm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xK1Muy8yjnc_"
      },
      "source": [
        "## Importing and Preprocessing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cEhfurzekX4q"
      },
      "outputs": [],
      "source": [
        "with open('/gdrive/MyDrive/unsupervised.pickle', 'rb') as f:\n",
        "    x_unsupervised = pk.load(f)\n",
        "\n",
        "with open('/gdrive/MyDrive/x_train.pickle', 'rb') as f:\n",
        "    x_train = pk.load(f)\n",
        "\n",
        "with open('/gdrive/MyDrive/x_val.pickle', 'rb') as f:\n",
        "    x_valid = pk.load(f)\n",
        "\n",
        "with open('/gdrive/MyDrive/x_test.pickle', 'rb') as f:\n",
        "    x_test = pk.load(f)\n",
        "\n",
        "y_train = np.loadtxt('/gdrive/MyDrive/y_train.txt', dtype='int32')\n",
        "y_valid = np.loadtxt('/gdrive/MyDrive/y_val.txt', dtype='int32')\n",
        "\n",
        "unwanted_digit=['0','1','2','3','4','5','6','7','8','9']\n",
        "\n",
        "for digit in unwanted_digit:\n",
        "    x_unsupervised = [sent.replace(digit, \"\") for sent in x_unsupervised]\n",
        "    x_train = [sent.replace(digit, \"\") for sent in x_train]\n",
        "    x_valid = [sent.replace(digit, \"\") for sent in x_valid]\n",
        "    x_test = [sent.replace(digit, \"\") for sent in x_test]\n",
        "\n",
        "unwanted_punc=['<br />','...',',','\"','=','@','&','%',',',':','\\\\','$','^','<','>','{','}',';','\\n','\\t','(',')','[',']','/','*','+','#','-','_','|']\n",
        "\n",
        "for punc in unwanted_punc:\n",
        "    x_unsupervised = [sent.replace(punc, \"\") for sent in x_unsupervised]\n",
        "    x_train = [sent.replace(punc, \"\") for sent in x_train]\n",
        "    x_valid = [sent.replace(punc, \"\") for sent in x_valid]\n",
        "    x_test = [sent.replace(punc, \"\") for sent in x_test]\n",
        "\n",
        "x_unsupervised = [sent.lower() for sent in x_unsupervised]\n",
        "x_train = [sent.lower() for sent in x_train]\n",
        "x_valid = [sent.lower() for sent in x_valid]\n",
        "x_test = [sent.lower() for sent in x_test]\n",
        "\n",
        "trainset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).map(lambda x, y: (x, tf.one_hot(y, 2))).batch(32)\n",
        "validset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).map(lambda x, y: (x, tf.one_hot(y, 2))).batch(32)\n",
        "testset = tf.data.Dataset.from_tensor_slices(x_test).batch(32)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "sUB_jmWEkA1v"
      },
      "source": [
        "## BPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ScJhOzeokpYY"
      },
      "outputs": [],
      "source": [
        "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset\n",
        "\n",
        "vocab = bert_vocab_from_dataset.bert_vocab_from_dataset(\n",
        "    tf.data.Dataset.from_tensor_slices(x_unsupervised),\n",
        "    vocab_size = 8000,\n",
        "    learn_params = {},\n",
        "    reserved_tokens = [\"[PAD]\", \"[UNK]\", \"[MASK]\", \"[START]\", \"[END]\"],\n",
        "    bert_tokenizer_params = dict(lower_case=True)\n",
        ")\n",
        "\n",
        "with open('/gdrive/MyDrive/vocab.txt', 'w') as f:\n",
        "    for token in vocab:\n",
        "        print(token, file=f)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "RcWorY6mk1RQ"
      },
      "source": [
        "## Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3JjYmiChJqpR"
      },
      "outputs": [],
      "source": [
        "class TransformerProcessor(tf.keras.layers.Layer):\n",
        "    def __init__(self, max_length=500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = tft.BertTokenizer('/gdrive/MyDrive/vocab.txt')\n",
        "        self.trimmer = tft.RoundRobinTrimmer(max_seq_length=max_length)\n",
        "\n",
        "    def call(self, x):\n",
        "        # input_type_ids, input_word_ids, input_mask\n",
        "        tokens = self.tokenizer.tokenize(x).merge_dims(1, -1)\n",
        "        tokens = self.trimmer.trim([tokens])[0]\n",
        "\n",
        "        words, mask = tft.pad_model_inputs(tokens, max_seq_length=self.max_length)\n",
        "\n",
        "        return {\n",
        "            \"input_mask\": tf.cast(mask, tf.int32),\n",
        "            \"input_word_ids\": tf.cast(words, tf.int32),\n",
        "            # \"input_type_ids\": tf.cast(tf.zeros(words.shape), tf.int32),\n",
        "        }\n",
        "\n",
        "class TransformerEncoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_size, num_layers, num_heads, dense_size, max_length=500):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_dim=8000, output_dim=embed_size, mask_zero=True)\n",
        "        self.position = tfm.nlp.layers.PositionEmbedding(max_length=max_length)\n",
        "        self.encoder = tfm.nlp.models.TransformerEncoder(num_layers=num_layers, num_attention_heads=num_heads, intermediate_size=dense_size)\n",
        "        self.pooler = tf.keras.layers.GlobalAveragePooling1D()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # input_type_ids, input_word_ids, input_mask\n",
        "        outputs = self.embedding(inputs[\"input_word_ids\"])\n",
        "        outputs = tf.keras.layers.add([outputs, self.position(outputs)])\n",
        "        outputs = self.encoder(outputs)\n",
        "        outputs = self.pooler(outputs)\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4R9pACYsMyka"
      },
      "source": [
        "#### Model 1 (EmbedSize: 128, NumLayer: 2, NumHead: 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AZK4V6KM07q",
        "outputId": "1138644d-9666-47d3-a879-34f600dfd8a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " transformer_processor (Tra  {'input_mask': (None, 500)   0         ['input_1[0][0]']             \n",
            " nsformerProcessor)          , 'input_word_ids': (None,                                           \n",
            "                              500)}                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder (Trans  (None, 128)                  1246308   ['transformer_processor[0][0]'\n",
            " formerEncoder)                                                     , 'transformer_processor[0][1]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dropout (Dropout)           (None, 128)                  0         ['transformer_encoder[0][0]'] \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 2)                    258       ['dropout[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1246566 (4.76 MB)\n",
            "Trainable params: 1246566 (4.76 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 212s 255ms/step - loss: 0.3934 - accuracy: 0.8138 - val_loss: 0.2901 - val_accuracy: 0.8806\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 78s 99ms/step - loss: 0.2102 - accuracy: 0.9182 - val_loss: 0.3473 - val_accuracy: 0.8593\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 80s 103ms/step - loss: 0.1520 - accuracy: 0.9423 - val_loss: 0.3803 - val_accuracy: 0.8552\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 78s 100ms/step - loss: 0.1011 - accuracy: 0.9630 - val_loss: 0.5180 - val_accuracy: 0.8522\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 78s 100ms/step - loss: 0.0703 - accuracy: 0.9746 - val_loss: 0.6223 - val_accuracy: 0.8571\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f3cb875eda0>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = tf.keras.Input(shape=(), dtype=tf.string)\n",
        "output = TransformerProcessor(max_length=500)(input)\n",
        "output = TransformerEncoder(embed_size=128, num_layers=2, num_heads=2, dense_size=50, max_length=500)(output)\n",
        "output = tf.keras.layers.Dropout(0.1)(output)\n",
        "output = tf.keras.layers.Dense(2, activation='softmax')(output)\n",
        "\n",
        "model_1 = tf.keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "model_1.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "print(model_1.summary())\n",
        "\n",
        "\n",
        "model_1.fit(trainset, validation_data=validset, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEvCX4sSiwdX",
        "outputId": "4f348ef9-f679-4346-cd70-8708105fe7fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 27s 70ms/step - loss: 0.6223 - accuracy: 0.8571\n",
            "[0.6222927570343018, 0.8571199774742126]\n"
          ]
        }
      ],
      "source": [
        "print(model_1.evaluate(validset))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2buM5Q0BMJcj"
      },
      "source": [
        "#### Model 2 (EmbedSize: 128, NumLayer: 4, NumHead: 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjCmFVkWMFEH",
        "outputId": "543c9a7d-2e91-4052-f237-ab568fc61e32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_2 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " transformer_processor_1 (T  {'input_mask': (None, 500)   0         ['input_2[0][0]']             \n",
            " ransformerProcessor)        , 'input_word_ids': (None,                                           \n",
            "                              500)}                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder_2 (Tra  (None, 128)                  1404360   ['transformer_processor_1[0][0\n",
            " nsformerEncoder)                                                   ]',                           \n",
            "                                                                     'transformer_processor_1[0][1\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)         (None, 128)                  0         ['transformer_encoder_2[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 2)                    258       ['dropout_1[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1404618 (5.36 MB)\n",
            "Trainable params: 1404618 (5.36 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 216s 255ms/step - loss: 0.4188 - accuracy: 0.7990 - val_loss: 0.2939 - val_accuracy: 0.8771\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 145s 185ms/step - loss: 0.2171 - accuracy: 0.9147 - val_loss: 0.3730 - val_accuracy: 0.8590\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 148s 189ms/step - loss: 0.1572 - accuracy: 0.9399 - val_loss: 0.3679 - val_accuracy: 0.8599\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 146s 187ms/step - loss: 0.1152 - accuracy: 0.9559 - val_loss: 0.4623 - val_accuracy: 0.8562\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 149s 190ms/step - loss: 0.0658 - accuracy: 0.9768 - val_loss: 0.5448 - val_accuracy: 0.8556\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f3ca8992740>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = tf.keras.Input(shape=(), dtype=tf.string)\n",
        "output = TransformerProcessor(max_length=500)(input)\n",
        "output = TransformerEncoder(embed_size=128, num_layers=4, num_heads=4, dense_size=50, max_length=500)(output)\n",
        "output = tf.keras.layers.Dropout(0.1)(output)\n",
        "output = tf.keras.layers.Dense(2, activation='softmax')(output)\n",
        "\n",
        "model_2 = tf.keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "model_2.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "print(model_2.summary())\n",
        "\n",
        "\n",
        "model_2.fit(trainset, validation_data=validset, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbUFrBlsTUUv",
        "outputId": "299d348b-3b4c-4e2f-af01-d40b9fdf3b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 28s 71ms/step - loss: 0.5448 - accuracy: 0.8556\n",
            "[0.544844388961792, 0.8555999994277954]\n"
          ]
        }
      ],
      "source": [
        "print(model_2.evaluate(validset))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fLmFLskpMtNd"
      },
      "source": [
        "#### Model 3 (EmbedSize: 128, NumLayer: 8, NumHead: 8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0Ei6ou9Mtkj",
        "outputId": "fece80b8-cd52-4175-d6ee-ffb86a722f1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " transformer_processor_2 (T  {'input_mask': (None, 500)   0         ['input_3[0][0]']             \n",
            " ransformerProcessor)        , 'input_word_ids': (None,                                           \n",
            "                              500)}                                                               \n",
            "                                                                                                  \n",
            " transformer_encoder_4 (Tra  (None, 128)                  1720464   ['transformer_processor_2[0][0\n",
            " nsformerEncoder)                                                   ]',                           \n",
            "                                                                     'transformer_processor_2[0][1\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)         (None, 128)                  0         ['transformer_encoder_4[0][0]'\n",
            "                                                                    ]                             \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 2)                    258       ['dropout_2[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1720722 (6.56 MB)\n",
            "Trainable params: 1720722 (6.56 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 378s 448ms/step - loss: 0.4281 - accuracy: 0.7889 - val_loss: 0.2892 - val_accuracy: 0.8801\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 351s 449ms/step - loss: 0.2066 - accuracy: 0.9197 - val_loss: 0.3731 - val_accuracy: 0.8646\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 325s 415ms/step - loss: 0.1387 - accuracy: 0.9482 - val_loss: 0.3836 - val_accuracy: 0.8657\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 348s 445ms/step - loss: 0.0867 - accuracy: 0.9691 - val_loss: 0.4801 - val_accuracy: 0.8582\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 318s 407ms/step - loss: 0.0875 - accuracy: 0.9670 - val_loss: 0.5002 - val_accuracy: 0.8572\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f3ca5c14970>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = tf.keras.Input(shape=(), dtype=tf.string)\n",
        "output = TransformerProcessor(max_length=500)(input)\n",
        "output = TransformerEncoder(embed_size=128, num_layers=8, num_heads=8, dense_size=50, max_length=500)(output)\n",
        "output = tf.keras.layers.Dropout(0.1)(output)\n",
        "output = tf.keras.layers.Dense(2, activation='softmax')(output)\n",
        "\n",
        "model_3 = tf.keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "model_3.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "print(model_3.summary())\n",
        "\n",
        "model_3.fit(trainset, validation_data=validset, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8VLU1G0xvnA",
        "outputId": "d67f7e00-87a1-4a32-eaba-03db858e9db2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 62s 158ms/step - loss: 0.5002 - accuracy: 0.8572\n",
            "[0.5002080798149109, 0.857200026512146]\n"
          ]
        }
      ],
      "source": [
        "print(model_3.evaluate(validset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6pMFLD6WKv6",
        "outputId": "70cebff2-b982-4d2b-fe00-1bdc27b788c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 17s 42ms/step\n"
          ]
        }
      ],
      "source": [
        "np.savetxt(\"/gdrive/MyDrive/y_test.txt\", np.argmax(model_1.predict(testset), axis=1), fmt=\"%d\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7GlwldtkNad1"
      },
      "source": [
        "## Masked LM BERT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "0-JRppTINeO8"
      },
      "outputs": [],
      "source": [
        "preprocessor = tfh.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "\n",
        "mask_id = preprocessor.tokenize.get_special_tokens_dict()[\"mask_id\"].numpy()\n",
        "padding_id = preprocessor.tokenize.get_special_tokens_dict()[\"padding_id\"].numpy()\n",
        "end_of_segment_id = preprocessor.tokenize.get_special_tokens_dict()[\"end_of_segment_id\"].numpy()\n",
        "start_of_sequence_id = preprocessor.tokenize.get_special_tokens_dict()[\"start_of_sequence_id\"].numpy()\n",
        "vocab_size = preprocessor.tokenize.get_special_tokens_dict()[\"vocab_size\"].numpy()\n",
        "\n",
        "selector = tft.RandomItemSelector(max_selections_per_batch=20, selection_rate=0.2, unselectable_ids=[mask_id, padding_id, end_of_segment_id, start_of_sequence_id])\n",
        "chooser = tft.MaskValuesChooser(vocab_size=vocab_size, mask_token=mask_id, mask_token_rate=0.8, random_token_rate=0.1)\n",
        "\n",
        "def preprocess(x):\n",
        "    input = preprocessor(x)\n",
        "\n",
        "    masked_input_ids, masked_lm_positions, masked_lm_ids = tft.mask_language_model(\n",
        "        input_ids=tf.RaggedTensor.from_tensor(input[\"input_word_ids\"]),\n",
        "        item_selector=selector,\n",
        "        mask_values_chooser=chooser\n",
        "    )\n",
        "\n",
        "    words, _ = tft.pad_model_inputs(masked_input_ids, max_seq_length=128)\n",
        "    positions, _ = tft.pad_model_inputs(masked_lm_positions, max_seq_length=20)\n",
        "    reals, _ = tft.pad_model_inputs(masked_lm_ids, max_seq_length=20)\n",
        "\n",
        "    input[\"input_word_ids\"] = tf.cast(words, tf.int32)\n",
        "    input[\"masked_lm_positions\"] = tf.cast(positions, tf.int32)\n",
        "\n",
        "    return (input, tf.one_hot(reals, vocab_size))\n",
        "\n",
        "mlmset = tf.data.Dataset.from_tensor_slices(x_unsupervised).batch(32).map(lambda x: preprocess(x))\n",
        "\n",
        "tf.saved_model.save(preprocessor, \"./bert_preprocessor\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ2akpxN8LM6",
        "outputId": "007c343f-9a5c-4ca6-bbea-f1de261767d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_mask (InputLayer)     [(None, 128)]                0         []                            \n",
            "                                                                                                  \n",
            " input_type_ids (InputLayer  [(None, 128)]                0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " input_word_ids (InputLayer  [(None, 128)]                0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " masked_lm_positions (Input  [(None, 20)]                 0         []                            \n",
            " Layer)                                                                                           \n",
            "                                                                                                  \n",
            " keras_layer_2 (KerasLayer)  {'sequence_output': (None,   4829754   ['input_mask[0][0]',          \n",
            "                              128, 128),                             'input_type_ids[0][0]',      \n",
            "                              'encoder_outputs': [(None              'input_word_ids[0][0]',      \n",
            "                             , 128, 128),                            'masked_lm_positions[0][0]'] \n",
            "                              (None, 128, 128),                                                   \n",
            "                              (None, 128, 128),                                                   \n",
            "                              (None, 128, 128)],                                                  \n",
            "                              'pooled_output': (None, 1                                           \n",
            "                             28),                                                                 \n",
            "                              'mlm_logits': (None, 20,                                            \n",
            "                             30522)}                                                              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4829754 (18.42 MB)\n",
            "Trainable params: 4829754 (18.42 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "1563/1563 [==============================] - 175s 101ms/step - loss: 7.1301\n",
            "Epoch 2/5\n",
            "1563/1563 [==============================] - 150s 96ms/step - loss: 2.6231\n",
            "Epoch 3/5\n",
            "1563/1563 [==============================] - 147s 94ms/step - loss: 2.0051\n",
            "Epoch 4/5\n",
            "1563/1563 [==============================] - 151s 97ms/step - loss: 1.4436\n",
            "Epoch 5/5\n",
            "1563/1563 [==============================] - 150s 96ms/step - loss: 0.9891\n"
          ]
        }
      ],
      "source": [
        "bert = tfh.load(\"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/2\")\n",
        "\n",
        "mask = tf.keras.Input(shape=(128), name=\"input_mask\", dtype=tf.int32)\n",
        "words = tf.keras.Input(shape=(128), name=\"input_word_ids\", dtype=tf.int32)\n",
        "types = tf.keras.Input(shape=(128), name=\"input_type_ids\", dtype=tf.int32)\n",
        "positions = tf.keras.Input(shape=(20), name=\"masked_lm_positions\", dtype=tf.int32)\n",
        "output = tfh.KerasLayer(bert.mlm, trainable=True)({\"input_mask\": mask, \"input_word_ids\": words, \"input_type_ids\": types, \"masked_lm_positions\": positions})\n",
        "\n",
        "model_mlm = tf.keras.Model(inputs=[mask, words, types, positions], outputs=output[\"mlm_logits\"])\n",
        "\n",
        "model_mlm.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\"\n",
        ")\n",
        "\n",
        "print(model_mlm.summary())\n",
        "\n",
        "model_mlm.fit(mlmset, epochs=5)\n",
        "\n",
        "tf.saved_model.save(bert, \"./bert_model\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eWSEDJW8NAHY"
      },
      "source": [
        "## Model BERT Non-Trainable (SeqLen: 128, NumLayer: 4, NumHead: 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1dr2GttWVyO",
        "outputId": "ae20b803-0c62-4ffb-f361-bd26a8a309ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_4 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " keras_layer_3 (KerasLayer)  {'input_type_ids': (None,    0         ['input_4[0][0]']             \n",
            "                             128),                                                                \n",
            "                              'input_word_ids': (None,                                            \n",
            "                             128),                                                                \n",
            "                              'input_mask': (None, 128)                                           \n",
            "                             }                                                                    \n",
            "                                                                                                  \n",
            " keras_layer_4 (KerasLayer)  {'sequence_output': (None,   4782465   ['keras_layer_3[0][0]',       \n",
            "                              128, 128),                             'keras_layer_3[0][1]',       \n",
            "                              'encoder_outputs': [(None              'keras_layer_3[0][2]']       \n",
            "                             , 128, 128),                                                         \n",
            "                              (None, 128, 128),                                                   \n",
            "                              (None, 128, 128),                                                   \n",
            "                              (None, 128, 128)],                                                  \n",
            "                              'pooled_output': (None, 1                                           \n",
            "                             28),                                                                 \n",
            "                              'default': (None, 128)}                                             \n",
            "                                                                                                  \n",
            " dropout_3 (Dropout)         (None, 128)                  0         ['keras_layer_4[0][5]']       \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 2)                    258       ['dropout_3[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4782723 (18.24 MB)\n",
            "Trainable params: 258 (1.01 KB)\n",
            "Non-trainable params: 4782465 (18.24 MB)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 126s 155ms/step - loss: 0.7083 - accuracy: 0.5029 - val_loss: 0.7010 - val_accuracy: 0.5041\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 123s 158ms/step - loss: 0.6234 - accuracy: 0.5221 - val_loss: 0.6530 - val_accuracy: 0.5251\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 116s 148ms/step - loss: 0.5516 - accuracy: 0.5706 - val_loss: 0.6223 - val_accuracy: 0.5301\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 123s 157ms/step - loss: 0.4951 - accuracy: 0.5907 - val_loss: 0.5826 - val_accuracy: 0.5415\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 120s 154ms/step - loss: 0.2141 - accuracy: 0.5909 - val_loss: 0.5546 - val_accuracy: 0.5561\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f3bc07906d0>"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = tf.keras.Input(shape=(), dtype=tf.string)\n",
        "output = tfh.KerasLayer(\"./bert_preprocessor\")(input)\n",
        "output = tfh.KerasLayer(\"./bert_model\", trainable=False)(output)\n",
        "output = tf.keras.layers.Dropout(0.1)(output[\"pooled_output\"])\n",
        "output = tf.keras.layers.Dense(2, activation='softmax')(output)\n",
        "\n",
        "model_4 = tf.keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "model_4.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "print(model_4.summary())\n",
        "\n",
        "model_4.fit(trainset, validation_data=validset, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Aja9lkoX3ri",
        "outputId": "016487dc-1e1e-423f-be1b-62415c1af3f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 25s 63ms/step\n"
          ]
        }
      ],
      "source": [
        "np.savetxt(\"/gdrive/MyDrive/y_test2.txt\", np.argmax(model_4.predict(testset), axis=1), fmt=\"%d\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_2MhUydBNTiL"
      },
      "source": [
        "## Model BERT Trainable (SeqLen: 128, NumLayer: 4, NumHead: 4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div dir=\"rtl\">\n",
        "در این قسمت مدل بخش 5 به عنوان استخراج کننده ویژگی به صورت قابل آموزش استفاده خواهد شد.\n",
        " </div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W3XK04OSNVNU",
        "outputId": "6687a69e-335f-45a6-82bf-c970b227165d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_5 (InputLayer)        [(None,)]                    0         []                            \n",
            "                                                                                                  \n",
            " keras_layer_5 (KerasLayer)  {'input_word_ids': (None,    0         ['input_5[0][0]']             \n",
            "                             128),                                                                \n",
            "                              'input_type_ids': (None,                                            \n",
            "                             128),                                                                \n",
            "                              'input_mask': (None, 128)                                           \n",
            "                             }                                                                    \n",
            "                                                                                                  \n",
            " keras_layer_6 (KerasLayer)  {'default': (None, 128),     4782465   ['keras_layer_5[0][0]',       \n",
            "                              'pooled_output': (None, 1              'keras_layer_5[0][1]',       \n",
            "                             28),                                    'keras_layer_5[0][2]']       \n",
            "                              'sequence_output': (None,                                           \n",
            "                              128, 128),                                                          \n",
            "                              'encoder_outputs': [(None                                           \n",
            "                             , 128, 128),                                                         \n",
            "                              (None, 128, 128),                                                   \n",
            "                              (None, 128, 128),                                                   \n",
            "                              (None, 128, 128)]}                                                  \n",
            "                                                                                                  \n",
            " dropout_4 (Dropout)         (None, 128)                  0         ['keras_layer_6[0][5]']       \n",
            "                                                                                                  \n",
            " dense_4 (Dense)             (None, 2)                    258       ['dropout_4[0][0]']           \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 4782723 (18.24 MB)\n",
            "Trainable params: 258 (1.01 KB)\n",
            "Non-trainable params: 4782465 (18.24 MB)\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 124s 152ms/step - loss: 0.7120 - accuracy: 0.8351 - val_loss: 0.2892 - val_accuracy: 0.8731\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 121s 154ms/step - loss: 0.5048 - accuracy: 0.8911 - val_loss: 0.3731 - val_accuracy: 0.8632\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 122s 156ms/step - loss: 0.1006 - accuracy: 0.9214 - val_loss: 0.3836 - val_accuracy: 0.8630\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 123s 157ms/step - loss: 0.0913 - accuracy: 0.9315 - val_loss: 0.3993 - val_accuracy: 0.8591\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 125s 159ms/step - loss: 0.0818 - accuracy: 0.9851 - val_loss: 0.4002 - val_accuracy: 0.8543\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f3b64d29630>"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input = tf.keras.Input(shape=(), dtype=tf.string)\n",
        "output = tfh.KerasLayer(\"./bert_preprocessor\")(input)\n",
        "output = tfh.KerasLayer(\"./bert_model\", trainable=False)(output)\n",
        "output = tf.keras.layers.Dropout(0.1)(output[\"pooled_output\"])\n",
        "output = tf.keras.layers.Dense(2, activation='softmax')(output)\n",
        "\n",
        "model_5 = tf.keras.Model(inputs=input, outputs=output)\n",
        "\n",
        "model_5.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "print(model_5.summary())\n",
        "\n",
        "model_5.fit(trainset, validation_data=validset, epochs=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVWkvnm8XfHK",
        "outputId": "f989c9c6-e56b-4479-9440-8eca637660f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "391/391 [==============================] - 57s 143ms/step\n"
          ]
        }
      ],
      "source": [
        "np.savetxt(\"/gdrive/MyDrive/y_test3.txt\", np.argmax(model_5.predict(testset), axis=1), fmt=\"%d\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
